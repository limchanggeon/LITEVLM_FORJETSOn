# LiteVLM on Jetson

NVIDIA Jetson Orinì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰ Vision-Language Model (VLM) ì¶”ë¡  ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

- **ì €ì§€ì—° ì¶”ë¡ **: FP8 ì–‘ìí™” ë° TensorRT ìµœì í™”ë¡œ ìµœëŒ€ 3.2ë°° ì†ë„ í–¥ìƒ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: 50% ë©”ëª¨ë¦¬ ì ˆê°, 4GB VRAMì—ì„œë„ 1B ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥
- **3ë‹¨ê³„ ìµœì í™” íŒŒì´í”„ë¼ì¸**:
  - Patch Selection Module: ì¤‘ìš” íŒ¨ì¹˜ë§Œ ì„ íƒí•˜ì—¬ ì¸ì½”ë” ì—°ì‚°ëŸ‰ 3ë°° ì ˆê°
  - Visual Token Compression: íŠ¹ì§• í† í° ì••ì¶•ìœ¼ë¡œ LLM ì…ë ¥ ê¸¸ì´ ê°ì†Œ
  - Speculative Decoding: ë³‘ë ¬ í† í° ìƒì„±ìœ¼ë¡œ 2~3ë°° ë””ì½”ë”© ì†ë„ í–¥ìƒ

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- NVIDIA Jetson Orin (AGX Orin, Orin NX, Orin Nano)
- ìµœì†Œ 8GB RAM ê¶Œì¥
- ìµœì†Œ 32GB ì €ì¥ê³µê°„

### ì†Œí”„íŠ¸ì›¨ì–´
- JetPack 5.1+ (CUDA 11.4+, TensorRT 8.5+)
- Python 3.8+
- PyTorch 2.0+

## ğŸ› ï¸ ì„¤ì¹˜ ë°©ë²•

### 1. ì €ì¥ì†Œ í´ë¡  (Jetsonì—ì„œ ì‹¤í–‰)

```bash
# Git Clone (ê¶Œì¥)
git clone https://github.com/limchanggeon/LITEVLM_FORJETSOn.git
cd LITEVLM_FORJETSOn

# ë˜ëŠ” ZIP ë‹¤ìš´ë¡œë“œ
wget https://github.com/limchanggeon/LITEVLM_FORJETSOn/archive/refs/heads/main.zip
unzip main.zip
cd LITEVLM_FORJETSOn-main
```

ğŸ’¡ **ìì„¸í•œ ì„¤ì¹˜ ê°€ì´ë“œ**: [INSTALL_GUIDE.md](INSTALL_GUIDE.md) ì°¸ê³ 

### 2. Conda í™˜ê²½ ì„¤ì •

```bash
# Conda í™˜ê²½ ìƒì„±
conda create -n litevlm python=3.10
conda activate litevlm

# ë˜ëŠ” venv ì‚¬ìš©
python3 -m venv venv
source venv/bin/activate
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# PyTorch ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Jetsonìš©)
pip install -r requirements.txt

# Jetsonì—ì„œ PyTorchëŠ” NVIDIAì—ì„œ ì œê³µí•˜ëŠ” wheel ì‚¬ìš© ê¶Œì¥
# https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048
```

### 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python scripts/download_models.py

# TensorRTë¡œ ë³€í™˜ (FP8 ì–‘ìí™” í¬í•¨)
python scripts/convert_to_tensorrt.py --fp8
```

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. Web UI (ê°€ì¥ ê°„ë‹¨! ğŸ¨)

```bash
# Gradio ê¸°ë°˜ ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
python webui.py

# íŠ¹ì • í¬íŠ¸ ì§€ì •
python webui.py --port 8080

# ì™¸ë¶€ ì ‘ì† í—ˆìš©
python webui.py --host 0.0.0.0 --port 7860

# Public ê³µìœ  ë§í¬ ìƒì„±
python webui.py --share
```

ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:7860` ì ‘ì†!

### 2. ê¸°ë³¸ ì¶”ë¡  (Python API)

```python
from litevlm import LiteVLM

# ëª¨ë¸ ì´ˆê¸°í™”
vlm = LiteVLM(
    vision_encoder="models/internvit_fp8.engine",
    text_decoder="models/qwen_fp8.engine",
    token_compression=True,
    speculative_decode=True
)

# ì´ë¯¸ì§€ ë¶„ì„
result = vlm.chat(
    image="example.jpg", 
    prompt="ì´ ì‚¬ì§„ì„ ì„¤ëª…í•´ì¤˜"
)
print(result)
```

### 3. ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤

```bash
python inference.py --image example.jpg --prompt "What is in this image?"
```

### 4. ë²¤ì¹˜ë§ˆí¬

```bash
# ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
python benchmark.py --model_path models/ --num_runs 100
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
liteVLM_injetson/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ models/                  # ë³€í™˜ëœ TensorRT ì—”ì§„ íŒŒì¼
â”œâ”€â”€ litevlm/                 # ë©”ì¸ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py            # LiteVLM í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ vision_encoder.py   # Vision ì¸ì½”ë” (InternViT)
â”‚   â”œâ”€â”€ text_decoder.py     # Text ë””ì½”ë” (Qwen2)
â”‚   â”œâ”€â”€ token_compression.py # í† í° ì••ì¶• ëª¨ë“ˆ
â”‚   â””â”€â”€ speculative_decode.py # Speculative decoding
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_models.py   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
â”‚   â”œâ”€â”€ convert_to_tensorrt.py # TensorRT ë³€í™˜
â”‚   â””â”€â”€ setup_jetson.sh      # Jetson ì´ˆê¸° ì„¤ì •
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_inference.py
â”‚   â””â”€â”€ batch_processing.py
â”œâ”€â”€ inference.py             # CLI ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ benchmark.py             # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```

## ğŸ¯ Jetson Orin ìµœì í™” íŒ

### 1. ì „ë ¥ ëª¨ë“œ ì„¤ì •
```bash
sudo nvpmodel -m 0  # MAX ì„±ëŠ¥ ëª¨ë“œ
sudo jetson_clocks   # í´ëŸ­ ê³ ì •
```

### 2. Swap ë©”ëª¨ë¦¬ í™•ì¥ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
```bash
sudo systemctl disable nvzramconfig
sudo fallocate -l 8G /mnt/8GB.swap
sudo mkswap /mnt/8GB.swap
sudo swapon /mnt/8GB.swap
```

### 3. TensorRT ìµœì í™” ë ˆë²¨
- FP16: ë†’ì€ ì •í™•ë„, ì¤‘ê°„ ì†ë„
- FP8: ê· í˜•ì¡íŒ ì„±ëŠ¥ (ê¶Œì¥)
- INT8: ìµœê³  ì†ë„, ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| í”Œë«í¼ | ëª¨ë¸ í¬ê¸° | ì§€ì—°ì‹œê°„ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|--------|----------|---------|-------------|
| Jetson Orin AGX | 1B (FP8) | ~45ms | 3.2GB |
| Jetson Orin NX | 1B (FP8) | ~68ms | 3.2GB |
| RTX 4070 | 1B (FP8) | ~28ms | 3.0GB |

*ì´ë¯¸ì§€ í¬ê¸°: 336Ã—336, í…ìŠ¤íŠ¸ ê¸¸ì´: í‰ê·  50 í† í° ê¸°ì¤€

## ğŸ”§ ë¬¸ì œ í•´ê²°

### CUDA Out of Memory
- ì‘ì€ ëª¨ë¸ ì‚¬ìš© (0.5B)
- ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
- Swap ë©”ëª¨ë¦¬ í™œì„±í™”

### TensorRT ë³€í™˜ ì‹¤íŒ¨
- JetPack ë²„ì „ í™•ì¸
- ONNX ëª¨ë¸ ë¨¼ì € ê²€ì¦
- ë¡œê·¸ í™•ì¸: `--verbose` í”Œë˜ê·¸ ì‚¬ìš©

## ğŸ“š ì°¸ê³  ìë£Œ

- [LiteVLM Paper](https://arxiv.org/abs/2501.xxxxx)
- [InternVL2.5 Documentation](https://github.com/OpenGVLab/InternVL)
- [Qwen2 Model Card](https://huggingface.co/Qwen)
- [NVIDIA TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆ ë° í’€ ë¦¬í€˜ìŠ¤íŠ¸ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“§ ë¬¸ì˜

í”„ë¡œì íŠ¸ì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì€ ì´ìŠˆë¥¼ í†µí•´ ë‚¨ê²¨ì£¼ì„¸ìš”.
